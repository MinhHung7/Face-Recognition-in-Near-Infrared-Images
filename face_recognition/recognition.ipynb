{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r /content/Face-recognition-in-Near-InfraRed-images/requirements.txt\n",
    "!pip install --upgrade opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chinmay/sem-six/ai-ml/face_detection/face_detection_and_recongition_using_ir_camera/face_recognition\n"
     ]
    }
   ],
   "source": [
    "# import all libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import ZeroPadding2D,Convolution2D,MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense,Dropout,Softmax,Flatten,Activation,BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import load_img,img_to_array\n",
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
    "import tensorflow.keras.backend as K\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "path=os.getcwd()\n",
    "face_detection_path=os.getcwd(),\"../\"\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing face detection libraries....\n",
      "now you can use face detection libraries\n"
     ]
    }
   ],
   "source": [
    "### importing all the face detetion methods from face_detection folder\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"/content/Face-recognition-in-Near-InfraRed-images/\")\n",
    "from face_detection.yoloface_face_detection import yoloface_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vgg face model's architecture\n",
    "model = Sequential()\n",
    "model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "model.add(ZeroPadding2D((1,1)))\t\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(2622, (1, 1)))\n",
    "model.add(Flatten())\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip /content/vgg_face_weights.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "zero_padding2d_13 (ZeroPaddi (None, 226, 226, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_14 (ZeroPaddi (None, 226, 226, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_15 (ZeroPaddi (None, 114, 114, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_16 (ZeroPaddi (None, 114, 114, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_17 (ZeroPaddi (None, 58, 58, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_18 (ZeroPaddi (None, 58, 58, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_19 (ZeroPaddi (None, 58, 58, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_20 (ZeroPaddi (None, 30, 30, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_21 (ZeroPaddi (None, 30, 30, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_22 (ZeroPaddi (None, 30, 30, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_23 (ZeroPaddi (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_24 (ZeroPaddi (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_25 (ZeroPaddi (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 1, 1, 4096)        102764544 \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1, 1, 4096)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 1, 1, 4096)        16781312  \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1, 1, 4096)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 1, 1, 2622)        10742334  \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2622)              0         \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2622)              0         \n",
      "=================================================================\n",
      "Total params: 145,002,878\n",
      "Trainable params: 145,002,878\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load vgg face model weights\n",
    "# Remove last Softmax layer and get model upto last flatten layer #with outputs 2622 units \n",
    "model.summary()\n",
    "vgg_face=Model(inputs=model.layers[0].input,outputs=model.layers[-2].output)\n",
    "model.load_weights('vgg_face_weights.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip dataset\n",
    "!unzip /content/TD_NIR_A_Set.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /content/Face-recognition-in-Near-InfraRed-images/face_detection/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = os.path.join(path, \"/content/yolo_cropped_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Định nghĩa augmentation\n",
    "transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.3),\n",
    "    A.MotionBlur(blur_limit=3, p=0.2),\n",
    "    A.Affine(scale=(0.95, 1.05), rotate=(-5, 5), translate_percent=(0.02, 0.05), p=0.4),\n",
    "    A.ToGray(p=1.0)  # Giữ ảnh ở dạng grayscale (NIR)\n",
    "])\n",
    "\n",
    "for folder in os.listdir(train_datasets):\n",
    "    # Thư mục chứa ảnh gốc\n",
    "    input_folder = os.path.join(train_datasets, folder)\n",
    "\n",
    "    # Lặp qua tất cả ảnh trong thư mục đầu vào\n",
    "    for filename in os.listdir(input_folder):\n",
    "        \n",
    "        if filename.startswith(\"AUG_\"):\n",
    "            continue\n",
    "\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        \n",
    "        # Đọc ảnh ở dạng grayscale\n",
    "        image = cv2.imread(input_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            print(f\"Không thể đọc {filename}, bỏ qua...\")\n",
    "            continue\n",
    "\n",
    "        # Augment ảnh\n",
    "        augmented = transform(image=image)['image']\n",
    "        \n",
    "        # Lưu ảnh đã xử lý\n",
    "        output_path = os.path.join(input_folder, \"AUG_\" + filename)\n",
    "        cv2.imwrite(output_path, augmented)\n",
    "        print(f\"Lưu ảnh augment: {output_path}\")\n",
    "\n",
    "print(\"Hoàn thành augmentation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: '/home/chinmay/sem-six/ai-ml/face_detection/face_detection_and_recongition_using_ir_camera/face_recognition/images_crop/subject_3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-90f224e5ad6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mcur_crop_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_crop_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mcur_test_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_test_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_crop_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_test_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/home/chinmay/sem-six/ai-ml/face_detection/face_detection_and_recongition_using_ir_camera/face_recognition/images_crop/subject_3'"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "### python script to automatically divide data from tested_dataset folder\n",
    "data_set_path=os.path.join(path,\"/content/yolo_cropped_images\")\n",
    "\n",
    "images_crop_path=os.path.join(path,\"/content/images_train_crop\")\n",
    "images_test_path=os.path.join(path,\"/content/images_test_crop\")\n",
    "\n",
    "for file in os.listdir(data_set_path):\n",
    "\n",
    "    ## create a folder inside images_crop and images_test_crop\n",
    "    cur_crop_path=os.path.join(images_crop_path,file)\n",
    "    cur_test_path=os.path.join(images_test_path,file)\n",
    "    os.makedirs(cur_crop_path, exist_ok=True)\n",
    "    os.makedirs(cur_test_path, exist_ok=True)\n",
    "\n",
    "    specific_dataset_path=os.path.join(data_set_path,file)\n",
    "    dataset=os.listdir(specific_dataset_path)\n",
    "\n",
    "    partition=(len(dataset)*7)//10\n",
    "\n",
    "    for i in range(partition):\n",
    "        shutil.copy(os.path.join(specific_dataset_path,dataset[i]), os.path.join(cur_crop_path,dataset[i]))\n",
    "    for i in range(partition,len(dataset)):\n",
    "        shutil.copy(os.path.join(specific_dataset_path,dataset[i]), os.path.join(cur_test_path,dataset[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject_3\n",
      "96.jpg\t100.jpg\t104.jpg\t109.jpg\t134.jpg\t85.jpg\t18.jpg\t55.jpg\t1.jpg\t73.jpg\t14.jpg\t84.jpg\t181.jpg\t56.jpg\t62.jpg\t94.jpg\t147.jpg\t77.jpg\t72.jpg\t37.jpg\t154.jpg\t58.jpg\t112.jpg\t12.jpg\t170.jpg\t102.jpg\t142.jpg\t116.jpg\t80.jpg\t16.jpg\t182.jpg\t108.jpg\t60.jpg\t81.jpg\t141.jpg\t155.jpg\t23.jpg\t69.jpg\t180.jpg\t163.jpg\t22.jpg\t38.jpg\t145.jpg\t143.jpg\t135.jpg\t187.jpg\t74.jpg\t30.jpg\t11.jpg\t186.jpg\t95.jpg\t0.jpg\t33.jpg\t91.jpg\t136.jpg\t127.jpg\t128.jpg\t4.jpg\t88.jpg\t21.jpg\t92.jpg\t46.jpg\t43.jpg\t133.jpg\t6.jpg\t70.jpg\t98.jpg\t54.jpg\t13.jpg\t169.jpg\t105.jpg\t34.jpg\t68.jpg\t76.jpg\t89.jpg\t113.jpg\t83.jpg\t152.jpg\t164.jpg\t99.jpg\t115.jpg\t65.jpg\t125.jpg\t3.jpg\t61.jpg\t51.jpg\t59.jpg\t124.jpg\t185.jpg\t90.jpg\t183.jpg\t24.jpg\t129.jpg\t157.jpg\t25.jpg\t93.jpg\t118.jpg\t165.jpg\t17.jpg\t168.jpg\t2.jpg\t144.jpg\t64.jpg\t28.jpg\t40.jpg\t5.jpg\t15.jpg\t50.jpg\t71.jpg\t107.jpg\t20.jpg\t8.jpg\t36.jpg\t110.jpg\t149.jpg\t78.jpg\t139.jpg\t138.jpg\t126.jpg\t166.jpg\t137.jpg\t57.jpg\t153.jpg\t82.jpg\t9.jpg\t106.jpg\t123.jpg\t48.jpg\t39.jpg\t160.jpg\t63.jpg\tsubject_9\n",
      "96.jpg\t100.jpg\t104.jpg\t109.jpg\t85.jpg\t18.jpg\t55.jpg\t1.jpg\t73.jpg\t14.jpg\t84.jpg\t56.jpg\t62.jpg\t94.jpg\t77.jpg\t72.jpg\t37.jpg\t58.jpg\t12.jpg\t102.jpg\t80.jpg\t16.jpg\t108.jpg\t60.jpg\t81.jpg\t23.jpg\t69.jpg\t22.jpg\t38.jpg\t74.jpg\t30.jpg\t11.jpg\t95.jpg\t0.jpg\t33.jpg\t91.jpg\t4.jpg\t88.jpg\t21.jpg\t92.jpg\t46.jpg\t43.jpg\t6.jpg\t70.jpg\t98.jpg\t54.jpg\t13.jpg\t105.jpg\t34.jpg\t68.jpg\t76.jpg\t89.jpg\t83.jpg\t99.jpg\t65.jpg\t3.jpg\t61.jpg\t51.jpg\t59.jpg\t90.jpg\t24.jpg\t25.jpg\t93.jpg\t17.jpg\t2.jpg\t64.jpg\t28.jpg\t40.jpg\t5.jpg\t15.jpg\t50.jpg\t71.jpg\t107.jpg\t20.jpg\t8.jpg\t36.jpg\t110.jpg\t78.jpg\tsubject_17\n",
      "195.jpg\t96.jpg\t100.jpg\t104.jpg\t109.jpg\t134.jpg\t85.jpg\t18.jpg\t241.jpg\t55.jpg\t1.jpg\t199.jpg\t73.jpg\t14.jpg\t84.jpg\t236.jpg\t181.jpg\t56.jpg\t62.jpg\t94.jpg\t147.jpg\t77.jpg\t72.jpg\t37.jpg\t154.jpg\t215.jpg\t58.jpg\t112.jpg\t12.jpg\t170.jpg\t102.jpg\t209.jpg\t142.jpg\t116.jpg\t219.jpg\t197.jpg\t80.jpg\t217.jpg\t16.jpg\t182.jpg\t108.jpg\t60.jpg\t81.jpg\t141.jpg\t155.jpg\t23.jpg\t69.jpg\t190.jpg\t204.jpg\t180.jpg\t163.jpg\t210.jpg\t22.jpg\t38.jpg\t145.jpg\t143.jpg\t135.jpg\t187.jpg\t224.jpg\t229.jpg\t228.jpg\t203.jpg\t216.jpg\t74.jpg\t30.jpg\t11.jpg\t186.jpg\t233.jpg\t95.jpg\t0.jpg\t33.jpg\t91.jpg\t136.jpg\t230.jpg\t127.jpg\t128.jpg\t221.jpg\t4.jpg\t245.jpg\t244.jpg\t88.jpg\t231.jpg\t21.jpg\t92.jpg\t238.jpg\t218.jpg\t227.jpg\t46.jpg\t43.jpg\t133.jpg\t191.jpg\t222.jpg\t6.jpg\t70.jpg\t98.jpg\t54.jpg\t13.jpg\t169.jpg\t105.jpg\t34.jpg\t68.jpg\t76.jpg\t89.jpg\t113.jpg\t226.jpg\t83.jpg\t152.jpg\t189.jpg\t243.jpg\t164.jpg\t213.jpg\t99.jpg\t115.jpg\t234.jpg\t65.jpg\t125.jpg\t3.jpg\t61.jpg\t51.jpg\t59.jpg\t124.jpg\t240.jpg\t185.jpg\t206.jpg\t205.jpg\t90.jpg\t183.jpg\t24.jpg\t129.jpg\t157.jpg\t25.jpg\t192.jpg\t214.jpg\t194.jpg\t242.jpg\t93.jpg\t118.jpg\t165.jpg\t237.jpg\t17.jpg\t168.jpg\t2.jpg\t220.jpg\t144.jpg\t64.jpg\t28.jpg\t40.jpg\t5.jpg\t15.jpg\t50.jpg\t71.jpg\t107.jpg\t20.jpg\t232.jpg\t8.jpg\t36.jpg\t110.jpg\t196.jpg\t149.jpg\t193.jpg\t78.jpg\t139.jpg\t138.jpg\t126.jpg\t166.jpg\t137.jpg\t57.jpg\t153.jpg\t82.jpg\t9.jpg\t106.jpg\t123.jpg\tsubject_14\n",
      "85.jpg\t18.jpg\t55.jpg\t1.jpg\t73.jpg\t14.jpg\t84.jpg\t56.jpg\t62.jpg\t94.jpg\t77.jpg\t72.jpg\t37.jpg\t58.jpg\t12.jpg\t80.jpg\t16.jpg\t60.jpg\t81.jpg\t23.jpg\t69.jpg\t22.jpg\t38.jpg\t74.jpg\t30.jpg\t11.jpg\t0.jpg\t33.jpg\t91.jpg\t4.jpg\t88.jpg\t21.jpg\t92.jpg\t46.jpg\t43.jpg\t6.jpg\t70.jpg\t54.jpg\t13.jpg\t34.jpg\t68.jpg\t76.jpg\t89.jpg\t83.jpg\t65.jpg\t3.jpg\t61.jpg\t51.jpg\t59.jpg\t90.jpg\t24.jpg\t25.jpg\t93.jpg\t17.jpg\t2.jpg\t64.jpg\t28.jpg\t40.jpg\t5.jpg\t15.jpg\t50.jpg\t71.jpg\t20.jpg\t8.jpg\t36.jpg\t78.jpg\tsubject_2\n",
      "18.jpg\t55.jpg\t1.jpg\t14.jpg\t56.jpg\t37.jpg\t12.jpg\t16.jpg\t23.jpg\t22.jpg\t38.jpg\t30.jpg\t11.jpg\t0.jpg\t33.jpg\t4.jpg\t21.jpg\t46.jpg\t43.jpg\t6.jpg\t54.jpg\t13.jpg\t34.jpg\t3.jpg\t51.jpg\t24.jpg\t25.jpg\t17.jpg\t2.jpg\t28.jpg\t40.jpg\t5.jpg\t15.jpg\t50.jpg\t20.jpg\t8.jpg\t36.jpg\t57.jpg\t9.jpg\t48.jpg\tsubject_10\n",
      "96.jpg\t100.jpg\t104.jpg\t109.jpg\t134.jpg\t85.jpg\t18.jpg\t55.jpg\t1.jpg\t73.jpg\t14.jpg\t84.jpg\t56.jpg\t62.jpg\t94.jpg\t147.jpg\t77.jpg\t72.jpg\t37.jpg\t154.jpg\t58.jpg\t112.jpg\t12.jpg\t102.jpg\t142.jpg\t116.jpg\t80.jpg\t16.jpg\t108.jpg\t60.jpg\t81.jpg\t141.jpg\t155.jpg\t23.jpg\t69.jpg\t22.jpg\t38.jpg\t145.jpg\t143.jpg\t135.jpg\t74.jpg\t30.jpg\t11.jpg\t95.jpg\t0.jpg\t33.jpg\t91.jpg\t136.jpg\t127.jpg\t128.jpg\t4.jpg\t88.jpg\t21.jpg\t92.jpg\t46.jpg\t43.jpg\t133.jpg\t6.jpg\t70.jpg\t98.jpg\t54.jpg\t13.jpg\t105.jpg\t34.jpg\t68.jpg\t76.jpg\t89.jpg\t113.jpg\t83.jpg\t152.jpg\t99.jpg\t115.jpg\t65.jpg\t125.jpg\t3.jpg\t61.jpg\t51.jpg\t59.jpg\t124.jpg\t90.jpg\t24.jpg\t129.jpg\t157.jpg\t25.jpg\t93.jpg\t118.jpg\t17.jpg\t2.jpg\t144.jpg\t64.jpg\t28.jpg\t40.jpg\t5.jpg\t15.jpg\t50.jpg\t71.jpg\t107.jpg\t20.jpg\t8.jpg\t36.jpg\t110.jpg\t149.jpg\t78.jpg\t139.jpg\t138.jpg\t126.jpg\t137.jpg\t57.jpg\t153.jpg\t82.jpg\tsubject_16\n",
      "96.jpg\t100.jpg\t104.jpg\t109.jpg\t134.jpg\t85.jpg\t18.jpg\t55.jpg\t1.jpg\t73.jpg\t14.jpg\t84.jpg\t181.jpg\t56.jpg\t62.jpg\t94.jpg\t147.jpg\t77.jpg\t72.jpg\t37.jpg\t154.jpg\t58.jpg\t112.jpg\t12.jpg\t170.jpg\t102.jpg\t142.jpg\t116.jpg\t80.jpg\t16.jpg\t182.jpg\t108.jpg\t60.jpg\t81.jpg\t141.jpg\t155.jpg\t23.jpg\t69.jpg\t180.jpg\t163.jpg\t22.jpg\t38.jpg\t145.jpg\t143.jpg\t135.jpg\t74.jpg\t30.jpg\t11.jpg\t186.jpg\t95.jpg\t0.jpg\t33.jpg\t91.jpg\t136.jpg\t127.jpg\t128.jpg\t4.jpg\t88.jpg\t21.jpg\t92.jpg\t46.jpg\t43.jpg\t133.jpg\t6.jpg\t70.jpg\t98.jpg\t54.jpg\t13.jpg\t169.jpg\t105.jpg\t34.jpg\t68.jpg\t76.jpg\t89.jpg\t113.jpg\t83.jpg\t152.jpg\t164.jpg\t99.jpg\t115.jpg\t65.jpg\t125.jpg\t3.jpg\t61.jpg\t51.jpg\t59.jpg\t124.jpg\t185.jpg\t90.jpg\t183.jpg\t24.jpg\t129.jpg\t157.jpg\t25.jpg\t93.jpg\t118.jpg\t165.jpg\t17.jpg\t168.jpg\t2.jpg\t144.jpg\t64.jpg\t28.jpg\t40.jpg\t5.jpg\t15.jpg\t50.jpg\t71.jpg\t107.jpg\t20.jpg\t8.jpg\t36.jpg\t110.jpg\t149.jpg\t78.jpg\t139.jpg\t138.jpg\t126.jpg\t166.jpg\t137.jpg\t57.jpg\t153.jpg\t82.jpg\t9.jpg\t106.jpg\t123.jpg\t48.jpg\t39.jpg\t160.jpg\t63.jpg\tsubject_11\n",
      "195.jpg\t96.jpg\t100.jpg\t104.jpg\t109.jpg\t134.jpg\t85.jpg\t18.jpg\t241.jpg\t55.jpg\t1.jpg\t199.jpg\t250.jpg\t73.jpg\t14.jpg\t84.jpg\t236.jpg\t181.jpg\t56.jpg\t62.jpg\t94.jpg\t147.jpg\t77.jpg\t72.jpg\t37.jpg\t154.jpg\t215.jpg\t58.jpg\t112.jpg\t12.jpg\t170.jpg\t102.jpg\t209.jpg\t142.jpg\t262.jpg\t116.jpg\t219.jpg\t197.jpg\t80.jpg\t217.jpg\t16.jpg\t268.jpg\t182.jpg\t256.jpg\t108.jpg\t60.jpg\t81.jpg\t141.jpg\t155.jpg\t23.jpg\t69.jpg\t190.jpg\t204.jpg\t180.jpg\t163.jpg\t210.jpg\t22.jpg\t38.jpg\t145.jpg\t143.jpg\t135.jpg\t187.jpg\t224.jpg\t229.jpg\t228.jpg\t203.jpg\t216.jpg\t74.jpg\t30.jpg\t11.jpg\t186.jpg\t233.jpg\t95.jpg\t252.jpg\t0.jpg\t33.jpg\t266.jpg\t269.jpg\t91.jpg\t136.jpg\t230.jpg\t127.jpg\t128.jpg\t221.jpg\t4.jpg\t245.jpg\t244.jpg\t88.jpg\t231.jpg\t21.jpg\t92.jpg\t238.jpg\t218.jpg\t227.jpg\t46.jpg\t255.jpg\t248.jpg\t43.jpg\t133.jpg\t191.jpg\t222.jpg\t6.jpg\t70.jpg\t98.jpg\t54.jpg\t13.jpg\t169.jpg\t257.jpg\t105.jpg\t34.jpg\t68.jpg\t76.jpg\t89.jpg\t113.jpg\t226.jpg\t83.jpg\t152.jpg\t189.jpg\t243.jpg\t164.jpg\t213.jpg\t99.jpg\t115.jpg\t234.jpg\t65.jpg\t125.jpg\t3.jpg\t61.jpg\t51.jpg\t59.jpg\t124.jpg\t258.jpg\t254.jpg\t240.jpg\t185.jpg\t206.jpg\t205.jpg\t90.jpg\t183.jpg\t24.jpg\t129.jpg\t157.jpg\t25.jpg\t192.jpg\t214.jpg\t264.jpg\t194.jpg\t242.jpg\t93.jpg\t118.jpg\t165.jpg\t237.jpg\t17.jpg\t168.jpg\t2.jpg\t220.jpg\t144.jpg\t64.jpg\t28.jpg\t40.jpg\t5.jpg\t15.jpg\t50.jpg\t71.jpg\t107.jpg\t20.jpg\t232.jpg\t8.jpg\t36.jpg\t110.jpg\t196.jpg\t149.jpg\t193.jpg\t78.jpg\t139.jpg\t138.jpg\t126.jpg\t166.jpg\t260.jpg\t137.jpg\t57.jpg\t153.jpg\t82.jpg\t9.jpg\t106.jpg\t123.jpg\t247.jpg\t48.jpg\t39.jpg\tsubject_4\n",
      "195.jpg\t96.jpg\t100.jpg\t104.jpg\t109.jpg\t134.jpg\t85.jpg\t18.jpg\t55.jpg\t1.jpg\t73.jpg\t14.jpg\t84.jpg\t181.jpg\t56.jpg\t62.jpg\t94.jpg\t147.jpg\t77.jpg\t72.jpg\t37.jpg\t154.jpg\t58.jpg\t112.jpg\t12.jpg\t170.jpg\t102.jpg\t142.jpg\t116.jpg\t197.jpg\t80.jpg\t16.jpg\t182.jpg\t108.jpg\t60.jpg\t81.jpg\t141.jpg\t155.jpg\t23.jpg\t69.jpg\t190.jpg\t180.jpg\t163.jpg\t22.jpg\t38.jpg\t145.jpg\t143.jpg\t135.jpg\t187.jpg\t74.jpg\t30.jpg\t11.jpg\t186.jpg\t95.jpg\t0.jpg\t33.jpg\t91.jpg\t136.jpg\t127.jpg\t128.jpg\t4.jpg\t88.jpg\t21.jpg\t92.jpg\t46.jpg\t43.jpg\t133.jpg\t191.jpg\t6.jpg\t70.jpg\t98.jpg\t54.jpg\t13.jpg\t169.jpg\t105.jpg\t34.jpg\t68.jpg\t76.jpg\t89.jpg\t113.jpg\t83.jpg\t152.jpg\t189.jpg\t164.jpg\t99.jpg\t115.jpg\t65.jpg\t125.jpg\t3.jpg\t61.jpg\t51.jpg\t59.jpg\t124.jpg\t185.jpg\t90.jpg\t183.jpg\t24.jpg\t129.jpg\t157.jpg\t25.jpg\t192.jpg\t194.jpg\t93.jpg\t118.jpg\t165.jpg\t17.jpg\t168.jpg\t2.jpg\t144.jpg\t64.jpg\t28.jpg\t40.jpg\t5.jpg\t15.jpg\t50.jpg\t71.jpg\t107.jpg\t20.jpg\t8.jpg\t36.jpg\t110.jpg\t196.jpg\t149.jpg\t193.jpg\t78.jpg\t139.jpg\t138.jpg\t126.jpg\t166.jpg\t137.jpg\t57.jpg\t153.jpg\t82.jpg\t9.jpg\t106.jpg\t123.jpg\t48.jpg\t39.jpg\tsubject_15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195.jpg\t282.jpg\t96.jpg\t100.jpg\t104.jpg\t109.jpg\t134.jpg\t85.jpg\t18.jpg\t241.jpg\t55.jpg\t1.jpg\t199.jpg\t289.jpg\t250.jpg\t73.jpg\t14.jpg\t84.jpg\t236.jpg\t181.jpg\t56.jpg\t62.jpg\t94.jpg\t147.jpg\t77.jpg\t72.jpg\t37.jpg\t154.jpg\t215.jpg\t58.jpg\t112.jpg\t12.jpg\t170.jpg\t102.jpg\t209.jpg\t142.jpg\t262.jpg\t116.jpg\t219.jpg\t197.jpg\t80.jpg\t217.jpg\t16.jpg\t268.jpg\t182.jpg\t256.jpg\t108.jpg\t280.jpg\t60.jpg\t81.jpg\t141.jpg\t155.jpg\t23.jpg\t69.jpg\t190.jpg\t204.jpg\t180.jpg\t294.jpg\t163.jpg\t210.jpg\t274.jpg\t22.jpg\t38.jpg\t145.jpg\t143.jpg\t135.jpg\t187.jpg\t224.jpg\t229.jpg\t228.jpg\t203.jpg\t216.jpg\t74.jpg\t30.jpg\t11.jpg\t186.jpg\t233.jpg\t95.jpg\t252.jpg\t0.jpg\t33.jpg\t266.jpg\t269.jpg\t293.jpg\t91.jpg\t136.jpg\t230.jpg\t276.jpg\t127.jpg\t285.jpg\t128.jpg\t221.jpg\t4.jpg\t245.jpg\t244.jpg\t88.jpg\t231.jpg\t21.jpg\t92.jpg\t238.jpg\t218.jpg\t227.jpg\t46.jpg\t255.jpg\t248.jpg\t43.jpg\t292.jpg\t133.jpg\t291.jpg\t191.jpg\t222.jpg\t6.jpg\t70.jpg\t98.jpg\t54.jpg\t13.jpg\t169.jpg\t273.jpg\t257.jpg\t105.jpg\t34.jpg\t68.jpg\t76.jpg\t89.jpg\t113.jpg\t226.jpg\t299.jpg\t83.jpg\t152.jpg\t189.jpg\t243.jpg\t284.jpg\t164.jpg\t213.jpg\t99.jpg\t115.jpg\t234.jpg\t65.jpg\t125.jpg\t3.jpg\t61.jpg\t51.jpg\t59.jpg\t124.jpg\t258.jpg\t254.jpg\t240.jpg\t185.jpg\t206.jpg\t205.jpg\t90.jpg\t183.jpg\t24.jpg\t129.jpg\t279.jpg\t157.jpg\t25.jpg\t192.jpg\t214.jpg\t264.jpg\t286.jpg\t194.jpg\t242.jpg\t93.jpg\t118.jpg\t165.jpg\t287.jpg\t277.jpg\t237.jpg\t17.jpg\t168.jpg\t2.jpg\t220.jpg\t144.jpg\t64.jpg\t28.jpg\t296.jpg\t40.jpg\t5.jpg\t15.jpg\t50.jpg\t71.jpg\t297.jpg\t107.jpg\t20.jpg\t232.jpg\t8.jpg\t36.jpg\t110.jpg\t196.jpg\t149.jpg\t193.jpg\t78.jpg\t139.jpg\t138.jpg\t126.jpg\t166.jpg\t260.jpg\t137.jpg\t57.jpg\t153.jpg\t82.jpg\t9.jpg\t106.jpg\t123.jpg\t247.jpg\t48.jpg\t39.jpg\t160.jpg\t207.jpg\tsubject_13\n",
      "195.jpg\t96.jpg\t100.jpg\t104.jpg\t109.jpg\t134.jpg\t85.jpg\t18.jpg\t241.jpg\t55.jpg\t1.jpg\t199.jpg\t250.jpg\t73.jpg\t14.jpg\t84.jpg\t236.jpg\t181.jpg\t56.jpg\t62.jpg\t94.jpg\t147.jpg\t77.jpg\t72.jpg\t37.jpg\t154.jpg\t215.jpg\t58.jpg\t112.jpg\t12.jpg\t170.jpg\t102.jpg\t209.jpg\t142.jpg\t116.jpg\t219.jpg\t197.jpg\t80.jpg\t217.jpg\t16.jpg\t182.jpg\t108.jpg\t60.jpg\t81.jpg\t141.jpg\t155.jpg\t23.jpg\t69.jpg\t190.jpg\t204.jpg\t180.jpg\t163.jpg\t210.jpg\t22.jpg\t38.jpg\t145.jpg\t143.jpg\t135.jpg\t187.jpg\t224.jpg\t229.jpg\t228.jpg\t203.jpg\t216.jpg\t74.jpg\t30.jpg\t11.jpg\t186.jpg\t233.jpg\t95.jpg\t252.jpg\t0.jpg\t33.jpg\t91.jpg\t136.jpg\t230.jpg\t127.jpg\t128.jpg\t221.jpg\t4.jpg\t245.jpg\t244.jpg\t88.jpg\t231.jpg\t21.jpg\t92.jpg\t238.jpg\t218.jpg\t227.jpg\t46.jpg\t248.jpg\t43.jpg\t133.jpg\t191.jpg\t222.jpg\t6.jpg\t70.jpg\t98.jpg\t54.jpg\t13.jpg\t169.jpg\t105.jpg\t34.jpg\t68.jpg\t76.jpg\t89.jpg\t113.jpg\t226.jpg\t83.jpg\t152.jpg\t189.jpg\t243.jpg\t164.jpg\t213.jpg\t99.jpg\t115.jpg\t234.jpg\t65.jpg\t125.jpg\t3.jpg\t61.jpg\t51.jpg\t59.jpg\t124.jpg\t254.jpg\t240.jpg\t185.jpg\t206.jpg\t205.jpg\t90.jpg\t183.jpg\t24.jpg\t129.jpg\t157.jpg\t25.jpg\t192.jpg\t214.jpg\t194.jpg\t242.jpg\t93.jpg\t118.jpg\t165.jpg\t237.jpg\t17.jpg\t168.jpg\t2.jpg\t220.jpg\t144.jpg\t64.jpg\t28.jpg\t40.jpg\t5.jpg\t15.jpg\t50.jpg\t71.jpg\t107.jpg\t20.jpg\t232.jpg\t8.jpg\t36.jpg\t110.jpg\t196.jpg\t149.jpg\t193.jpg\t78.jpg\t139.jpg\t138.jpg\t126.jpg\t166.jpg\t137.jpg\t57.jpg\t153.jpg\t82.jpg\t9.jpg\t106.jpg\t123.jpg\t247.jpg\t48.jpg\t{0: 'subject_3', 1: 'subject_9', 2: 'subject_17', 3: 'subject_14', 4: 'subject_2', 5: 'subject_10', 6: 'subject_16', 7: 'subject_11', 8: 'subject_4', 9: 'subject_15', 10: 'subject_13'}\n"
     ]
    }
   ],
   "source": [
    "# Prepare Training Data\n",
    "x_train=[]\n",
    "y_train=[]\n",
    "person_rep=dict()\n",
    "person_folders=os.listdir('/content/images_train_crop')\n",
    "for i,person in enumerate(person_folders):\n",
    "  person_rep[i]=person\n",
    "  image_names=os.listdir('/content/images_train_crop/'+person+'/')\n",
    "  print(person_rep[i])\n",
    "  for image_name in image_names:\n",
    "    print(image_name,end=\"\\t\")\n",
    "    img=load_img('/content/images_train_crop/'+person+'/'+image_name,target_size=(224,224))\n",
    "    img=img_to_array(img)\n",
    "    img=np.expand_dims(img,axis=0)\n",
    "    img=preprocess_input(img)\n",
    "    img_encode=vgg_face(img)\n",
    "    x_train.append(np.squeeze(K.eval(img_encode)).tolist())\n",
    "    y_train.append(i)\n",
    "\n",
    "\n",
    "print(person_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'subject_3',\n",
       " 1: 'subject_9',\n",
       " 2: 'subject_17',\n",
       " 3: 'subject_14',\n",
       " 4: 'subject_2',\n",
       " 5: 'subject_10',\n",
       " 6: 'subject_16',\n",
       " 7: 'subject_11',\n",
       " 8: 'subject_4',\n",
       " 9: 'subject_15',\n",
       " 10: 'subject_13'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject_3\n",
      "131.jpg\t79.jpg\t29.jpg\t31.jpg\t27.jpg\t114.jpg\t75.jpg\t148.jpg\t103.jpg\t32.jpg\t41.jpg\t167.jpg\t175.jpg\t119.jpg\t176.jpg\t162.jpg\t159.jpg\t158.jpg\t47.jpg\t121.jpg\t42.jpg\t45.jpg\t172.jpg\t87.jpg\t184.jpg\t53.jpg\t86.jpg\t178.jpg\t10.jpg\t19.jpg\t171.jpg\t130.jpg\t111.jpg\t35.jpg\t7.jpg\t26.jpg\t151.jpg\t67.jpg\t156.jpg\t140.jpg\t174.jpg\t120.jpg\t173.jpg\t150.jpg\t177.jpg\t161.jpg\t132.jpg\t52.jpg\t49.jpg\t179.jpg\t117.jpg\t101.jpg\t44.jpg\t66.jpg\t122.jpg\t146.jpg\t97.jpg\t\n",
      "subject_9\n",
      "57.jpg\t82.jpg\t9.jpg\t106.jpg\t48.jpg\t39.jpg\t63.jpg\t79.jpg\t29.jpg\t31.jpg\t27.jpg\t75.jpg\t103.jpg\t32.jpg\t41.jpg\t47.jpg\t42.jpg\t45.jpg\t87.jpg\t53.jpg\t86.jpg\t10.jpg\t19.jpg\t111.jpg\t35.jpg\t7.jpg\t26.jpg\t67.jpg\t52.jpg\t49.jpg\t101.jpg\t44.jpg\t66.jpg\t97.jpg\t\n",
      "subject_17\n",
      "48.jpg\t39.jpg\t160.jpg\t207.jpg\t63.jpg\t131.jpg\t79.jpg\t29.jpg\t31.jpg\t27.jpg\t114.jpg\t75.jpg\t148.jpg\t103.jpg\t32.jpg\t41.jpg\t167.jpg\t175.jpg\t119.jpg\t176.jpg\t208.jpg\t188.jpg\t162.jpg\t159.jpg\t158.jpg\t47.jpg\t121.jpg\t42.jpg\t45.jpg\t198.jpg\t172.jpg\t87.jpg\t184.jpg\t53.jpg\t201.jpg\t86.jpg\t178.jpg\t235.jpg\t10.jpg\t19.jpg\t225.jpg\t200.jpg\t171.jpg\t130.jpg\t111.jpg\t35.jpg\t7.jpg\t26.jpg\t151.jpg\t67.jpg\t156.jpg\t140.jpg\t174.jpg\t120.jpg\t173.jpg\t150.jpg\t177.jpg\t161.jpg\t212.jpg\t132.jpg\t239.jpg\t52.jpg\t49.jpg\t223.jpg\t179.jpg\t202.jpg\t117.jpg\t101.jpg\t44.jpg\t66.jpg\t122.jpg\t211.jpg\t146.jpg\t97.jpg\t\n",
      "subject_14\n",
      "57.jpg\t82.jpg\t9.jpg\t48.jpg\t39.jpg\t63.jpg\t79.jpg\t29.jpg\t31.jpg\t27.jpg\t75.jpg\t32.jpg\t41.jpg\t47.jpg\t42.jpg\t45.jpg\t87.jpg\t53.jpg\t86.jpg\t10.jpg\t19.jpg\t35.jpg\t7.jpg\t26.jpg\t67.jpg\t52.jpg\t49.jpg\t44.jpg\t66.jpg\t\n",
      "subject_2\n",
      "39.jpg\t29.jpg\t31.jpg\t27.jpg\t32.jpg\t41.jpg\t47.jpg\t42.jpg\t45.jpg\t53.jpg\t10.jpg\t19.jpg\t35.jpg\t7.jpg\t26.jpg\t52.jpg\t49.jpg\t44.jpg\t\n",
      "subject_10\n",
      "9.jpg\t106.jpg\t123.jpg\t48.jpg\t39.jpg\t63.jpg\t131.jpg\t79.jpg\t29.jpg\t31.jpg\t27.jpg\t114.jpg\t75.jpg\t148.jpg\t103.jpg\t32.jpg\t41.jpg\t119.jpg\t47.jpg\t121.jpg\t42.jpg\t45.jpg\t87.jpg\t53.jpg\t86.jpg\t10.jpg\t19.jpg\t130.jpg\t111.jpg\t35.jpg\t7.jpg\t26.jpg\t151.jpg\t67.jpg\t156.jpg\t140.jpg\t120.jpg\t150.jpg\t132.jpg\t52.jpg\t49.jpg\t117.jpg\t101.jpg\t44.jpg\t66.jpg\t122.jpg\t146.jpg\t97.jpg\t\n",
      "subject_16\n",
      "131.jpg\t79.jpg\t29.jpg\t31.jpg\t27.jpg\t114.jpg\t75.jpg\t148.jpg\t103.jpg\t32.jpg\t41.jpg\t167.jpg\t175.jpg\t119.jpg\t176.jpg\t162.jpg\t159.jpg\t158.jpg\t47.jpg\t121.jpg\t42.jpg\t45.jpg\t172.jpg\t87.jpg\t184.jpg\t53.jpg\t86.jpg\t178.jpg\t10.jpg\t19.jpg\t171.jpg\t130.jpg\t111.jpg\t35.jpg\t7.jpg\t26.jpg\t151.jpg\t67.jpg\t156.jpg\t140.jpg\t174.jpg\t120.jpg\t173.jpg\t150.jpg\t177.jpg\t161.jpg\t132.jpg\t52.jpg\t49.jpg\t179.jpg\t117.jpg\t101.jpg\t44.jpg\t66.jpg\t122.jpg\t146.jpg\t97.jpg\t\n",
      "subject_11\n",
      "160.jpg\t207.jpg\t249.jpg\t63.jpg\t131.jpg\t79.jpg\t29.jpg\t31.jpg\t27.jpg\t114.jpg\t75.jpg\t148.jpg\t103.jpg\t259.jpg\t32.jpg\t41.jpg\t167.jpg\t246.jpg\t263.jpg\t175.jpg\t119.jpg\t176.jpg\t208.jpg\t188.jpg\t162.jpg\t159.jpg\t158.jpg\t47.jpg\t121.jpg\t42.jpg\t45.jpg\t198.jpg\t172.jpg\t87.jpg\t184.jpg\t53.jpg\t201.jpg\t267.jpg\t86.jpg\t178.jpg\t235.jpg\t10.jpg\t19.jpg\t225.jpg\t200.jpg\t171.jpg\t130.jpg\t111.jpg\t35.jpg\t251.jpg\t7.jpg\t26.jpg\t151.jpg\t265.jpg\t270.jpg\t261.jpg\t253.jpg\t67.jpg\t156.jpg\t140.jpg\t174.jpg\t120.jpg\t173.jpg\t150.jpg\t177.jpg\t161.jpg\t212.jpg\t132.jpg\t239.jpg\t52.jpg\t49.jpg\t223.jpg\t179.jpg\t202.jpg\t117.jpg\t101.jpg\t44.jpg\t66.jpg\t122.jpg\t211.jpg\t146.jpg\t97.jpg\t\n",
      "subject_4\n",
      "160.jpg\t63.jpg\t131.jpg\t79.jpg\t29.jpg\t31.jpg\t27.jpg\t114.jpg\t75.jpg\t148.jpg\t103.jpg\t32.jpg\t41.jpg\t167.jpg\t175.jpg\t119.jpg\t176.jpg\t188.jpg\t162.jpg\t159.jpg\t158.jpg\t47.jpg\t121.jpg\t42.jpg\t45.jpg\t172.jpg\t87.jpg\t184.jpg\t53.jpg\t86.jpg\t178.jpg\t10.jpg\t19.jpg\t171.jpg\t130.jpg\t111.jpg\t35.jpg\t7.jpg\t26.jpg\t151.jpg\t67.jpg\t156.jpg\t140.jpg\t174.jpg\t120.jpg\t173.jpg\t150.jpg\t177.jpg\t161.jpg\t132.jpg\t52.jpg\t49.jpg\t179.jpg\t117.jpg\t101.jpg\t44.jpg\t66.jpg\t122.jpg\t146.jpg\t97.jpg\t\n",
      "subject_15\n",
      "249.jpg\t63.jpg\t131.jpg\t79.jpg\t29.jpg\t31.jpg\t27.jpg\t114.jpg\t75.jpg\t148.jpg\t103.jpg\t259.jpg\t32.jpg\t41.jpg\t167.jpg\t246.jpg\t298.jpg\t263.jpg\t175.jpg\t119.jpg\t288.jpg\t176.jpg\t281.jpg\t208.jpg\t188.jpg\t162.jpg\t159.jpg\t158.jpg\t47.jpg\t121.jpg\t42.jpg\t45.jpg\t198.jpg\t172.jpg\t87.jpg\t184.jpg\t53.jpg\t201.jpg\t275.jpg\t267.jpg\t283.jpg\t86.jpg\t178.jpg\t235.jpg\t10.jpg\t19.jpg\t225.jpg\t200.jpg\t171.jpg\t130.jpg\t111.jpg\t35.jpg\t251.jpg\t7.jpg\t290.jpg\t278.jpg\t26.jpg\t151.jpg\t272.jpg\t265.jpg\t270.jpg\t261.jpg\t253.jpg\t67.jpg\t156.jpg\t140.jpg\t174.jpg\t120.jpg\t271.jpg\t173.jpg\t150.jpg\t177.jpg\t161.jpg\t212.jpg\t132.jpg\t239.jpg\t295.jpg\t52.jpg\t49.jpg\t223.jpg\t179.jpg\t202.jpg\t117.jpg\t101.jpg\t44.jpg\t66.jpg\t122.jpg\t211.jpg\t146.jpg\t97.jpg\t\n",
      "subject_13\n",
      "39.jpg\t160.jpg\t207.jpg\t249.jpg\t63.jpg\t131.jpg\t79.jpg\t29.jpg\t31.jpg\t27.jpg\t114.jpg\t75.jpg\t148.jpg\t103.jpg\t32.jpg\t41.jpg\t167.jpg\t246.jpg\t175.jpg\t119.jpg\t176.jpg\t208.jpg\t188.jpg\t162.jpg\t159.jpg\t158.jpg\t47.jpg\t121.jpg\t42.jpg\t45.jpg\t198.jpg\t172.jpg\t87.jpg\t184.jpg\t53.jpg\t201.jpg\t86.jpg\t178.jpg\t235.jpg\t10.jpg\t19.jpg\t225.jpg\t200.jpg\t171.jpg\t130.jpg\t111.jpg\t35.jpg\t251.jpg\t7.jpg\t26.jpg\t151.jpg\t253.jpg\t67.jpg\t156.jpg\t140.jpg\t174.jpg\t120.jpg\t173.jpg\t150.jpg\t177.jpg\t161.jpg\t212.jpg\t132.jpg\t239.jpg\t52.jpg\t49.jpg\t223.jpg\t179.jpg\t202.jpg\t117.jpg\t101.jpg\t44.jpg\t66.jpg\t122.jpg\t211.jpg\t146.jpg\t97.jpg\t\n"
     ]
    }
   ],
   "source": [
    "#Prepare Test Data\n",
    "x_test=[]\n",
    "y_test=[]\n",
    "person_folders=os.listdir('/content/images_test_crop')\n",
    "for i,person in enumerate(person_folders):\n",
    "  image_names=os.listdir('/content/images_test_crop/'+person)\n",
    "  print(person)\n",
    "  for image_name in image_names:\n",
    "    print(image_name,end=\"\\t\")\n",
    "    img=load_img('/content/images_test_crop/'+person+'/'+image_name,target_size=(224,224))\n",
    "    img=img_to_array(img)\n",
    "    img=np.expand_dims(img,axis=0)\n",
    "    img=preprocess_input(img)\n",
    "    img_encode=vgg_face(img)\n",
    "    x_test.append(np.squeeze(K.eval(img_encode)).tolist())\n",
    "    y_test.append(i)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=np.array(x_train)\n",
    "y_train=np.array(y_train)\n",
    "x_test=np.array(x_test)\n",
    "y_test=np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test and train data for later use\n",
    "\n",
    "np.save('train_data_yolo_16_faces',x_train)\n",
    "np.save('train_labels_yolo_16_faces',y_train)\n",
    "np.save('test_data_yolo_16_faces',x_test)\n",
    "np.save('test_labels_yolo_16_faces',y_test)\n",
    "\n",
    "\n",
    "\n",
    "x_train=np.load('train_data_yolo_16_faces.npy')\n",
    "y_train=np.load('train_labels_yolo_16_faces.npy')\n",
    "x_test=np.load('test_data_yolo_16_faces.npy')\n",
    "y_test=np.load('test_labels_yolo_16_faces.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1442, 2622)\n"
     ]
    }
   ],
   "source": [
    "# Softmax regressor to classify images based on encoding\n",
    "print(x_train.shape)\n",
    "classifier_model=Sequential()\n",
    "classifier_model.add(Dense(units=100,input_dim=x_train.shape[1],kernel_initializer='glorot_uniform'))\n",
    "classifier_model.add(BatchNormalization())\n",
    "classifier_model.add(Activation('tanh'))\n",
    "classifier_model.add(Dropout(0.3))\n",
    "classifier_model.add(Dense(units=6,kernel_initializer='glorot_uniform'))\n",
    "classifier_model.add(BatchNormalization())\n",
    "classifier_model.add(Activation('tanh'))\n",
    "classifier_model.add(Dropout(0.2))\n",
    "classifier_model.add(Dense(units=16,kernel_initializer='he_uniform'))\n",
    "classifier_model.add(Activation('softmax'))\n",
    "classifier_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),optimizer='nadam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/72\n",
      "46/46 [==============================] - 2s 7ms/step - loss: 1.4187 - accuracy: 0.6990 - val_loss: 1.2124 - val_accuracy: 0.8339\n",
      "Epoch 2/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.1378 - accuracy: 0.8190 - val_loss: 0.8095 - val_accuracy: 0.9617\n",
      "Epoch 3/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 1.0252 - accuracy: 0.8675 - val_loss: 0.7562 - val_accuracy: 0.9952\n",
      "Epoch 4/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.9147 - accuracy: 0.8953 - val_loss: 0.6761 - val_accuracy: 0.9984\n",
      "Epoch 5/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.8110 - accuracy: 0.9119 - val_loss: 0.5617 - val_accuracy: 0.9984\n",
      "Epoch 6/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.7650 - accuracy: 0.9147 - val_loss: 0.5379 - val_accuracy: 1.0000\n",
      "Epoch 7/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.6957 - accuracy: 0.9105 - val_loss: 0.4726 - val_accuracy: 1.0000\n",
      "Epoch 8/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.6476 - accuracy: 0.9258 - val_loss: 0.4394 - val_accuracy: 1.0000\n",
      "Epoch 9/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.5998 - accuracy: 0.9293 - val_loss: 0.4169 - val_accuracy: 1.0000\n",
      "Epoch 10/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.5655 - accuracy: 0.9327 - val_loss: 0.3878 - val_accuracy: 1.0000\n",
      "Epoch 11/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.5364 - accuracy: 0.9265 - val_loss: 0.3377 - val_accuracy: 0.9968\n",
      "Epoch 12/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.4968 - accuracy: 0.9307 - val_loss: 0.3066 - val_accuracy: 0.9984\n",
      "Epoch 13/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.4862 - accuracy: 0.9355 - val_loss: 0.3142 - val_accuracy: 1.0000\n",
      "Epoch 14/72\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.4624 - accuracy: 0.9244 - val_loss: 0.2606 - val_accuracy: 1.0000\n",
      "Epoch 15/72\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.4307 - accuracy: 0.9417 - val_loss: 0.2319 - val_accuracy: 0.9984\n",
      "Epoch 16/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.4150 - accuracy: 0.9411 - val_loss: 0.2357 - val_accuracy: 1.0000\n",
      "Epoch 17/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3925 - accuracy: 0.9362 - val_loss: 0.2154 - val_accuracy: 1.0000\n",
      "Epoch 18/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3722 - accuracy: 0.9438 - val_loss: 0.1995 - val_accuracy: 1.0000\n",
      "Epoch 19/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3706 - accuracy: 0.9438 - val_loss: 0.1905 - val_accuracy: 1.0000\n",
      "Epoch 20/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3595 - accuracy: 0.9362 - val_loss: 0.2164 - val_accuracy: 1.0000\n",
      "Epoch 21/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3498 - accuracy: 0.9417 - val_loss: 0.1826 - val_accuracy: 1.0000\n",
      "Epoch 22/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3478 - accuracy: 0.9362 - val_loss: 0.1811 - val_accuracy: 1.0000\n",
      "Epoch 23/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3406 - accuracy: 0.9300 - val_loss: 0.1766 - val_accuracy: 1.0000\n",
      "Epoch 24/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3391 - accuracy: 0.9307 - val_loss: 0.1966 - val_accuracy: 1.0000\n",
      "Epoch 25/72\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.3357 - accuracy: 0.9286 - val_loss: 0.1550 - val_accuracy: 0.9984\n",
      "Epoch 26/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3056 - accuracy: 0.9369 - val_loss: 0.1297 - val_accuracy: 0.9984\n",
      "Epoch 27/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3090 - accuracy: 0.9348 - val_loss: 0.1377 - val_accuracy: 0.9984\n",
      "Epoch 28/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.3070 - accuracy: 0.9369 - val_loss: 0.1213 - val_accuracy: 0.9968\n",
      "Epoch 29/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2950 - accuracy: 0.9327 - val_loss: 0.1129 - val_accuracy: 0.9984\n",
      "Epoch 30/72\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.2714 - accuracy: 0.9459 - val_loss: 0.1019 - val_accuracy: 1.0000\n",
      "Epoch 31/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2783 - accuracy: 0.9438 - val_loss: 0.1075 - val_accuracy: 0.9984\n",
      "Epoch 32/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2702 - accuracy: 0.9480 - val_loss: 0.1864 - val_accuracy: 0.9952\n",
      "Epoch 33/72\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.2660 - accuracy: 0.9424 - val_loss: 0.0980 - val_accuracy: 0.9984\n",
      "Epoch 34/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2618 - accuracy: 0.9383 - val_loss: 0.0886 - val_accuracy: 0.9968\n",
      "Epoch 35/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2431 - accuracy: 0.9515 - val_loss: 0.0851 - val_accuracy: 1.0000\n",
      "Epoch 36/72\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.2469 - accuracy: 0.9521 - val_loss: 0.0859 - val_accuracy: 0.9984\n",
      "Epoch 37/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2483 - accuracy: 0.9424 - val_loss: 0.0812 - val_accuracy: 0.9984\n",
      "Epoch 38/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2445 - accuracy: 0.9521 - val_loss: 0.0817 - val_accuracy: 0.9984\n",
      "Epoch 39/72\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.2444 - accuracy: 0.9445 - val_loss: 0.0718 - val_accuracy: 1.0000\n",
      "Epoch 40/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2282 - accuracy: 0.9480 - val_loss: 0.0713 - val_accuracy: 1.0000\n",
      "Epoch 41/72\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.2237 - accuracy: 0.9501 - val_loss: 0.0682 - val_accuracy: 0.9984\n",
      "Epoch 42/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2208 - accuracy: 0.9438 - val_loss: 0.0870 - val_accuracy: 0.9952\n",
      "Epoch 43/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2235 - accuracy: 0.9508 - val_loss: 0.0695 - val_accuracy: 1.0000\n",
      "Epoch 44/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2257 - accuracy: 0.9473 - val_loss: 0.0684 - val_accuracy: 1.0000\n",
      "Epoch 45/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2093 - accuracy: 0.9515 - val_loss: 0.0571 - val_accuracy: 1.0000\n",
      "Epoch 46/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2159 - accuracy: 0.9459 - val_loss: 0.0547 - val_accuracy: 1.0000\n",
      "Epoch 47/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2281 - accuracy: 0.9383 - val_loss: 0.0627 - val_accuracy: 1.0000\n",
      "Epoch 48/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2261 - accuracy: 0.9397 - val_loss: 0.0598 - val_accuracy: 0.9968\n",
      "Epoch 49/72\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.2120 - accuracy: 0.9466 - val_loss: 0.0640 - val_accuracy: 0.9968\n",
      "Epoch 50/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2097 - accuracy: 0.9549 - val_loss: 0.0563 - val_accuracy: 0.9968\n",
      "Epoch 51/72\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.2063 - accuracy: 0.9459 - val_loss: 0.0490 - val_accuracy: 1.0000\n",
      "Epoch 52/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.1987 - accuracy: 0.9487 - val_loss: 0.0480 - val_accuracy: 0.9984\n",
      "Epoch 53/72\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.1863 - accuracy: 0.9646 - val_loss: 0.0501 - val_accuracy: 0.9968\n",
      "Epoch 54/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.1888 - accuracy: 0.9570 - val_loss: 0.0450 - val_accuracy: 0.9984\n",
      "Epoch 55/72\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.1846 - accuracy: 0.9528 - val_loss: 0.0437 - val_accuracy: 1.0000\n",
      "Epoch 56/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.1930 - accuracy: 0.9445 - val_loss: 0.0468 - val_accuracy: 1.0000\n",
      "Epoch 57/72\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.1942 - accuracy: 0.9542 - val_loss: 0.0423 - val_accuracy: 1.0000\n",
      "Epoch 58/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.2002 - accuracy: 0.9494 - val_loss: 0.0509 - val_accuracy: 0.9984\n",
      "Epoch 59/72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 5ms/step - loss: 0.1844 - accuracy: 0.9570 - val_loss: 0.0412 - val_accuracy: 1.0000\n",
      "Epoch 60/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.1794 - accuracy: 0.9570 - val_loss: 0.0660 - val_accuracy: 0.9936\n",
      "Epoch 61/72\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.1812 - accuracy: 0.9487 - val_loss: 0.0428 - val_accuracy: 1.0000\n",
      "Epoch 62/72\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.1827 - accuracy: 0.9563 - val_loss: 0.0362 - val_accuracy: 1.0000\n",
      "Epoch 63/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.1767 - accuracy: 0.9570 - val_loss: 0.0354 - val_accuracy: 1.0000\n",
      "Epoch 64/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.1591 - accuracy: 0.9619 - val_loss: 0.0342 - val_accuracy: 1.0000\n",
      "Epoch 65/72\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.1732 - accuracy: 0.9577 - val_loss: 0.0353 - val_accuracy: 1.0000\n",
      "Epoch 66/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.1728 - accuracy: 0.9535 - val_loss: 0.0351 - val_accuracy: 0.9984\n",
      "Epoch 67/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.1822 - accuracy: 0.9466 - val_loss: 0.0401 - val_accuracy: 1.0000\n",
      "Epoch 68/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.1710 - accuracy: 0.9521 - val_loss: 0.0342 - val_accuracy: 0.9984\n",
      "Epoch 69/72\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 0.1703 - accuracy: 0.9632 - val_loss: 0.0352 - val_accuracy: 0.9984\n",
      "Epoch 70/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.1535 - accuracy: 0.9612 - val_loss: 0.0333 - val_accuracy: 0.9984\n",
      "Epoch 71/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.1696 - accuracy: 0.9556 - val_loss: 0.0292 - val_accuracy: 1.0000\n",
      "Epoch 72/72\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.1597 - accuracy: 0.9591 - val_loss: 0.0275 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f30b7068df0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model with the training dataset\n",
    "classifier_model.fit(x_train,y_train,epochs=150,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.models.save_model(classifier_model,path+'/face_classifier_model_yolo_16_faces.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model=tf.keras.models.load_model(path+'/face_classifier_model_yolo_16_faces.h5')#16 people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(img):\n",
    "  plt.figure(figsize=(8,4))\n",
    "  plt.imshow(img[:,:,::-1])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yolov3-tiny_face.weights:: status : file already exists\n",
      "yolov3_tiny_face.cfg:: status : file already exists\n",
      "face_detection.weights:: status : file already exists\n",
      "face_detection.cfg:: status : file already exists\n"
     ]
    }
   ],
   "source": [
    "from yoloface import face_analysis\n",
    "face = face_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab.patches import cv2_imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Định nghĩa model nhận diện khuôn mặt và phân loại danh tính\n",
    "def detect_faces(image_path, output_path, face, vgg_face, classifier_model, person_rep):\n",
    "    # Đọc ảnh\n",
    "    frame = cv2.imread(image_path)\n",
    "    if frame is None:\n",
    "        print(f\"Không tìm thấy ảnh: {image_path}\")\n",
    "        return\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Phát hiện khuôn mặt\n",
    "    __, box, conf = face.face_detection(frame_arr=frame, frame_status=True, model='full')\n",
    "    faces_rect = np.array([np.array(xi) for xi in box])\n",
    "\n",
    "    for (x, y, w, h) in faces_rect:\n",
    "        cv2.rectangle(frame, (x, y), (x+h, y+w), (0, 255, 0), thickness=2)\n",
    "\n",
    "        # Cắt vùng khuôn mặt\n",
    "        # img_crop = gray[y:y+w, x:x+h]\n",
    "        img_crop = frame[y:y+w, x:x+h]\n",
    "\n",
    "        try:\n",
    "            cv2.imwrite(output_path + \"/face.jpg\", img_crop)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # Tiền xử lý ảnh\n",
    "        crop_img = load_img(output_path + \"/face.jpg\", target_size=(224, 224))\n",
    "        crop_img = img_to_array(crop_img)\n",
    "        crop_img = np.expand_dims(crop_img, axis=0)\n",
    "        crop_img = preprocess_input(crop_img)\n",
    "\n",
    "        # Trích xuất đặc trưng\n",
    "        img_encode = vgg_face(crop_img)\n",
    "        embed = K.eval(img_encode)\n",
    "\n",
    "        # Dự đoán danh tính\n",
    "        person = classifier_model.predict(embed)\n",
    "        name = person_rep[np.argmax(person)]\n",
    "        print(name)\n",
    "\n",
    "        # Vẽ kết quả lên ảnh\n",
    "        cv2.rectangle(gray, (x, y), (x+h, y+w), (0, 255, 0), 2)\n",
    "        if np.max(person) > 0.80:\n",
    "            cv2.putText(gray, name, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(gray, str(np.max(person)), (x, y + 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "    # Hiển thị ảnh đã xử lý\n",
    "    cv2_imshow(gray)\n",
    "    cv2.imwrite(output_path + \"/detected_faces.jpg\", gray)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Sử dụng hàm\n",
    "detect_faces(\"/content/images_test_crop/3/AUG_12.jpg\", \"/content/output\", face, vgg_face, classifier_model, person_rep)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "d9767bb80f0c926f2be51240ea78c27eb3c7b52dd704c89ce42f210d283b4c5b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
